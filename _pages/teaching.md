---
title: "Mrinmaya's Lab - Teaching"
layout: textlay
excerpt: "Teaching"
sitemap: false
permalink: /teaching
---

# Computational Semantics for Natural Language Processing
### <font color=gray>ETH Zürich, Spring Semester 2021</font>: [Course catalog](http://www.vvz.ethz.ch/lerneinheitPre.do?semkez=2021S&lerneinheitId=154041&lang=en)

___

## Course Description
This course presents an introduction to Natural language processing (NLP) with an emphasis on computational semantics i.e. the process of constructing and reasoning with meaning representations of natural language text.

The objective of the course is to learn about various topics in computational semantics and its importance in natural language processing methodology and research. Exercises and the project will be key parts of the course so the students will be able to gain hands-on experience with state-of-the-art techniques in the field.

___

### **Grading**
The final assessment will be a combination of classroom participation, graded exercises and the project. There will be 3 exercise sets which will be a mix of theoretical and implementation problems. Exercises will be released roughly every 4 weeks, and will total to 30% of your grade. Classroom participation (including a research paper presentation) will account for 20% of the grade. The project will account of the rest of the grade (50%). There will be no written exams.

**Lectures:** Thu 10-12h Zoom (TBD)

**Discussion Sections:**  Thu 15-16 Zoom (TBD)

**Textbooks:** TBD

## News
**08.02**    Class website is online!

___

## Course Schedule

<table border=0 cellpadding=0 cellspacing=0 width=1265 style='border-collapse:
 collapse;table-layout:fixed;width:949pt'>
 <col width=97 style='mso-width-source:userset;mso-width-alt:3104;width:73pt'>
 <col width=343 style='mso-width-source:userset;mso-width-alt:10976;width:257pt'>
 <col class=xl65 width=467 style='mso-width-source:userset;mso-width-alt:14944;
 width:350pt'>
 <col width=142 style='mso-width-source:userset;mso-width-alt:4544;width:107pt'>
 <col width=72 span=3 style='width:54pt'>
 <tr height=21 style='height:15.75pt'>
  <td height=21 class=xl66 width=97 style='height:15.75pt;width:73pt'>Lecture</td>
  <td class=xl67 width=343 style='width:257pt'>Description</td>
  <td class=xl122 width=467 style='width:350pt'>Course Materials</td>
  <td class=xl68 width=142 style='width:107pt'>Events</td>
  <td class=xl69 width=72 style='width:54pt'></td>
  <td class=xl69 width=72 style='width:54pt'></td>
  <td class=xl69 width=72 style='width:54pt'></td>
 </tr>
 <tr height=20 style='height:15.0pt'>
  <td rowspan=4 height=107 class=xl116 style='border-bottom:.5pt solid black;
  height:80.25pt'>1</td>
  <td rowspan=4 class=xl98 width=343 style='border-bottom:.5pt solid black;
  width:257pt'>Introduction<br>
    <font class="font11">The Distributional Hypothesis and Word Vectors</font></td>
  <td class=xl121 width=467 style='width:350pt'>Suggested Readings:</td>
  <td rowspan=4 class=xl116 style='border-bottom:.5pt solid black'>Assignment 1
  out</td>
  <td class=xl69></td>
  <td class=xl69></td>
  <td class=xl69></td>
 </tr>
 <tr height=19 style='height:14.25pt'>
  <td height=19 class=xl70 width=467 style='height:14.25pt;width:350pt'><a
  href="http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/"
  target="_parent"><span style='font-size:10.0pt'>1. Word2Vec Tutorial - The
  Skip-Gram Model</span></a></td>
  <td class=xl69></td>
  <td class=xl69></td>
  <td class=xl69></td>
 </tr>
 <tr height=34 style='height:25.5pt'>
  <td height=34 class=xl70 width=467 style='height:25.5pt;width:350pt'><a
  href="http://arxiv.org/pdf/1301.3781.pdf" target="_parent"><span
  style='font-size:10.0pt'>2. Efficient Estimation of Word Representations in
  Vector Space (original word2vec paper)</span></a></td>
  <td class=xl69></td>
  <td class=xl69></td>
  <td class=xl69></td>
 </tr>
 <tr height=34 style='height:25.5pt'>
  <td height=34 class=xl71 width=467 style='height:25.5pt;width:350pt'><a
  href="http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf"
  target="_parent"><span style='font-size:10.0pt'>3. Distributed
  Representations of Words and Phrases and their Compositionality (negative
  sampling paper)</span></a></td>
  <td class=xl69></td>
  <td class=xl69></td>
  <td class=xl69></td>
 </tr>
 <tr height=20 style='height:15.0pt'>
  <td rowspan=8 height=169 class=xl89 style='border-bottom:.5pt solid black;
  height:126.75pt;border-top:none'>2</td>
  <td class=xl72 width=343 style='width:257pt'>¡¡</td>
  <td class=xl123 width=467 style='width:350pt'>Suggested Readings:</td>
  <td rowspan=8 class=xl95 style='border-bottom:.5pt solid black;border-top:
  none'>¡¡</td>
  <td class=xl69></td>
  <td class=xl69></td>
  <td class=xl69></td>
 </tr>
 <tr height=19 style='height:14.25pt'>
  <td height=19 class=xl73 width=343 style='height:14.25pt;width:257pt'>¡¡</td>
  <td class=xl70 width=467 style='width:350pt'><a
  href="http://nlp.stanford.edu/pubs/glove.pdf" target="_parent"><span
  style='font-size:10.0pt'>1. GloVe: Global Vectors for Word Representation
  (original GloVe paper)</span></a></td>
  <td class=xl69></td>
  <td class=xl69></td>
  <td class=xl69></td>
 </tr>
 <tr height=19 style='height:14.25pt'>
  <td height=19 class=xl74 width=343 style='height:14.25pt;width:257pt'>¡¡</td>
  <td class=xl70 width=467 style='width:350pt'><a
  href="http://www.aclweb.org/anthology/D15-1036" target="_parent"><span
  style='font-size:10.0pt'>2. Evaluation methods for unsupervised word
  embeddings</span></a></td>
  <td class=xl69></td>
  <td class=xl69></td>
  <td class=xl69></td>
 </tr>
 <tr height=20 style='height:15.0pt'>
  <td height=20 class=xl72 width=343 style='height:15.0pt;width:257pt'>Word
  Vectors 2, Word Senses and Sentence Vectors</td>
  <td class=xl123 width=467 style='width:350pt'>Additional Readings:</td>
  <td class=xl69></td>
  <td class=xl69></td>
  <td class=xl69></td>
 </tr>
 <tr height=19 style='height:14.25pt'>
  <td height=19 class=xl73 width=343 style='height:14.25pt;width:257pt'>(Recursive
  Neural Networks)</td>
  <td class=xl70 width=467 style='width:350pt'><a
  href="http://aclweb.org/anthology/Q16-1028" target="_parent"><span
  style='font-size:10.0pt'>1. A Latent Variable Model Approach to PMI-based
  Word Embeddings</span></a></td>
  <td class=xl69></td>
  <td class=xl69></td>
  <td class=xl69></td>
 </tr>
 <tr height=34 style='height:25.5pt'>
  <td height=34 class=xl75 width=343 style='height:25.5pt;width:257pt'>¡¡</td>
  <td class=xl70 width=467 style='width:350pt'><a
  href="https://transacl.org/ojs/index.php/tacl/article/viewFile/1346/320"
  target="_parent"><span style='font-size:10.0pt'>2. Linear Algebraic Structure
  of Word Senses, with Applications to Polysemy</span></a></td>
  <td class=xl69></td>
  <td class=xl69></td>
  <td class=xl69></td>
 </tr>
 <tr height=19 style='height:14.25pt'>
  <td height=19 class=xl75 width=343 style='height:14.25pt;width:257pt'>¡¡</td>
  <td class=xl70 width=467 style='width:350pt'><a
  href="https://papers.nips.cc/paper/7368-on-the-dimensionality-of-word-embedding.pdf"
  target="_parent"><span style='font-size:10.0pt'>3. On the Dimensionality of
  Word Embedding.</span></a></td>
  <td class=xl69></td>
  <td class=xl69></td>
  <td class=xl69></td>
 </tr>
 <tr height=19 style='height:14.25pt'>
  <td height=19 class=xl76 width=343 style='height:14.25pt;width:257pt'>¡¡</td>
  <td class=xl71 width=467 style='width:350pt'><a
  href="https://web.stanford.edu/~jurafsky/slp3/19.pdf" target="_parent"><span
  style='font-size:10.0pt'>4. Word senses and Word embeddings chapter in
  Jurafsky and Martin</span></a></td>
  <td class=xl69></td>
  <td class=xl69></td>
  <td class=xl77></td>
 </tr>
 <tr height=20 style='mso-height-source:userset;height:15.0pt'>
  <td rowspan=4 height=78 class=xl117 style='border-bottom:.5pt solid black;
  height:58.5pt;border-top:none'>Voluntary</td>
  <td rowspan=4 class=xl120 width=343 style='border-bottom:.5pt solid black;
  border-top:none;width:257pt'>Python, Pytorch and Tensorflow review session by
  TAs</td>
  <td class=xl124 width=467 style='width:350pt'>Suggested Readings:</td>
  <td rowspan=4 class=xl113 style='border-bottom:.5pt solid black;border-top:
  none'>¡¡</td>
  <td class=xl69></td>
  <td class=xl69></td>
  <td class=xl69></td>
 </tr>
 <tr height=19 style='mso-height-source:userset;height:14.25pt'>
  <td height=19 class=xl78 width=467 style='height:14.25pt;width:350pt'><a
  href="http://web.stanford.edu/class/cs224n/readings/review-differential-calculus.pdf"
  target="_parent"><span style='font-size:10.0pt'>1. Review of differential
  calculus</span></a></td>
  <td class=xl69></td>
  <td class=xl69></td>
  <td class=xl69></td>
 </tr>
 <tr height=20 style='mso-height-source:userset;height:15.0pt'>
  <td height=20 class=xl124 width=467 style='height:15.0pt;width:350pt'>Additional
  Readings:</td>
  <td class=xl69></td>
  <td class=xl69></td>
  <td class=xl69></td>
 </tr>
 <tr height=19 style='mso-height-source:userset;height:14.25pt'>
  <td height=19 class=xl79 width=467 style='height:14.25pt;width:350pt'><a
  href="http://www.jmlr.org/papers/volume12/collobert11a/collobert11a.pdf"
  target="_parent"><span style='font-size:10.0pt'>1. Natural Language
  Processing (Almost) from Scratch</span></a></td>
  <td class=xl69></td>
  <td class=xl69></td>
  <td class=xl69></td>
 </tr>
 <tr height=20 style='mso-height-source:userset;height:15.0pt'>
  <td rowspan=6 height=115 class=xl107 style='border-bottom:.5pt solid black;
  height:86.25pt;border-top:none'>¡¡</td>
  <td rowspan=6 class=xl110 width=343 style='border-bottom:.5pt solid black;
  border-top:none;width:257pt'>Matrix Calculus and Backpropagation by TAs</td>
  <td class=xl124 width=467 style='width:350pt'>Suggested Readings:</td>
  <td rowspan=6 class=xl113 style='border-bottom:.5pt solid black;border-top:
  none'>¡¡</td>
  <td class=xl69></td>
  <td class=xl69></td>
  <td class=xl69></td>
 </tr>
 <tr height=19 style='mso-height-source:userset;height:14.25pt'>
  <td height=19 class=xl78 width=467 style='height:14.25pt;width:350pt'><a
  href="http://cs231n.github.io/neural-networks-1/" target="_parent"><span
  style='font-size:10.0pt'>1. CS231n notes on network architectures</span></a></td>
  <td class=xl69></td>
  <td class=xl69></td>
  <td class=xl69></td>
 </tr>
 <tr height=19 style='mso-height-source:userset;height:14.25pt'>
  <td height=19 class=xl78 width=467 style='height:14.25pt;width:350pt'><a
  href="http://cs231n.github.io/optimization-2/" target="_parent"><span
  style='font-size:10.0pt'>2. CS231n notes on backprop</span></a></td>
  <td class=xl69></td>
  <td class=xl69></td>
  <td class=xl69></td>
 </tr>
 <tr height=19 style='mso-height-source:userset;height:14.25pt'>
  <td height=19 class=xl78 width=467 style='height:14.25pt;width:350pt'><a
  href="http://www.iro.umontreal.ca/~vincentp/ift3395/lectures/backprop_old.pdf"
  target="_parent"><span style='font-size:10.0pt'>3. Learning Representations
  by Backpropagating Errors</span></a></td>
  <td class=xl69></td>
  <td class=xl69></td>
  <td class=xl69></td>
 </tr>
 <tr height=19 style='mso-height-source:userset;height:14.25pt'>
  <td height=19 class=xl78 width=467 style='height:14.25pt;width:350pt'><a
  href="http://cs231n.stanford.edu/handouts/derivatives.pdf" target="_parent"><span
  style='font-size:10.0pt'>4. Derivatives, Backpropagation, and Vectorization</span></a></td>
  <td class=xl69></td>
  <td class=xl69></td>
  <td class=xl69></td>
 </tr>
 <tr height=19 style='mso-height-source:userset;height:14.25pt'>
  <td height=19 class=xl79 width=467 style='height:14.25pt;width:350pt'><a
  href="https://medium.com/@karpathy/yes-you-should-understand-backprop-e2f06eab496b"
  target="_parent"><span style='font-size:10.0pt'>5. Yes you should understand
  backprop</span></a></td>
  <td class=xl69></td>
  <td class=xl69></td>
  <td class=xl69></td>
 </tr>
 <tr height=20 style='height:15.0pt'>
  <td rowspan=10 height=281 class=xl89 style='border-bottom:.5pt solid black;
  height:210.75pt;border-top:none'>3</td>
  <td class=xl72 width=343 style='width:257pt'>¡¡</td>
  <td class=xl123 width=467 style='width:350pt'>Suggested Readings:</td>
  <td rowspan=10 class=xl95 style='border-bottom:.5pt solid black;border-top:
  none'>¡¡</td>
  <td class=xl69></td>
  <td class=xl69></td>
  <td class=xl69></td>
 </tr>
 <tr height=19 style='height:14.25pt'>
  <td height=19 class=xl73 width=343 style='height:14.25pt;width:257pt'>¡¡</td>
  <td class=xl70 width=467 style='width:350pt'><a
  href="https://web.stanford.edu/~jurafsky/slp3/3.pdf" target="_parent"><span
  style='font-size:10.0pt'>1. N-gram Language Models (textbook chapter)</span></a></td>
  <td class=xl69></td>
  <td class=xl69></td>
  <td class=xl69></td>
 </tr>
 <tr height=34 style='height:25.5pt'>
  <td height=34 class=xl75 width=343 style='height:25.5pt;width:257pt'>¡¡</td>
  <td class=xl70 width=467 style='width:350pt'><a
  href="http://karpathy.github.io/2015/05/21/rnn-effectiveness/"
  target="_parent"><span style='font-size:10.0pt'>2. The Unreasonable
  Effectiveness of Recurrent Neural Networks (blog post overview)</span></a></td>
  <td class=xl69></td>
  <td class=xl69></td>
  <td class=xl69></td>
 </tr>
 <tr height=34 style='height:25.5pt'>
  <td height=34 class=xl75 width=343 style='height:25.5pt;width:257pt'>¡¡</td>
  <td class=xl70 width=467 style='width:350pt'><a
  href="http://www.deeplearningbook.org/contents/rnn.html" target="_parent"><span
  style='font-size:10.0pt'>3. Sequence Modeling: Recurrent and Recursive Neural
  Nets (Sections 10.1 and 10.2)</span></a></td>
  <td class=xl69></td>
  <td class=xl69></td>
  <td class=xl69></td>
 </tr>
 <tr height=34 style='height:25.5pt'>
  <td height=34 class=xl72 width=343 style='height:25.5pt;width:257pt'>From
  words to sentences<br>
    <font class="font10">Recurrent Neural Networks for Language</font></td>
  <td class=xl123 width=467 style='width:350pt'>Suggested Readings (RNNs):</td>
  <td class=xl69></td>
  <td class=xl69></td>
  <td class=xl69></td>
 </tr>
 <tr height=34 style='height:25.5pt'>
  <td height=34 class=xl73 width=343 style='height:25.5pt;width:257pt'>Case
  Study: Language Modelling</td>
  <td class=xl70 width=467 style='width:350pt'><a
  href="http://www.deeplearningbook.org/contents/rnn.html" target="_parent"><span
  style='font-size:10.0pt'>1. Sequence Modeling: Recurrent and Recursive Neural
  Nets (Sections 10.3, 10.5, 10.7-10.12)</span></a></td>
  <td class=xl69></td>
  <td class=xl69></td>
  <td class=xl69></td>
 </tr>
 <tr height=34 style='height:25.5pt'>
  <td height=34 class=xl75 width=343 style='height:25.5pt;width:257pt'>¡¡</td>
  <td class=xl70 width=467 style='width:350pt'><a
  href="http://ai.dinfo.unifi.it/paolo/ps/tnn-94-gradient.pdf" target="_parent"><span
  style='font-size:10.0pt'>2. Learning long-term dependencies with gradient
  descent is difficult (one of the original vanishing gradient papers)</span></a></td>
  <td class=xl69></td>
  <td class=xl69></td>
  <td class=xl69></td>
 </tr>
 <tr height=34 style='height:25.5pt'>
  <td height=34 class=xl75 width=343 style='height:25.5pt;width:257pt'>¡¡</td>
  <td class=xl70 width=467 style='width:350pt'><a
  href="https://arxiv.org/pdf/1211.5063.pdf" target="_parent"><span
  style='font-size:10.0pt'>3. On the difficulty of training Recurrent Neural
  Networks (proof of vanishing gradient problem)</span></a></td>
  <td class=xl69></td>
  <td class=xl69></td>
  <td class=xl69></td>
 </tr>
 <tr height=19 style='height:14.25pt'>
  <td height=19 class=xl75 width=343 style='height:14.25pt;width:257pt'>¡¡</td>
  <td class=xl70 width=467 style='width:350pt'><a
  href="https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1174/lectures/vanishing_grad_example.html"
  target="_parent"><span style='font-size:10.0pt'>4. Vanishing Gradients
  Jupyter Notebook (demo for feedforward networks)</span></a></td>
  <td class=xl69></td>
  <td class=xl69></td>
  <td class=xl69></td>
 </tr>
 <tr height=19 style='height:14.25pt'>
  <td height=19 class=xl76 width=343 style='height:14.25pt;width:257pt'>¡¡</td>
  <td class=xl71 width=467 style='width:350pt'><a
  href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/"
  target="_parent"><span style='font-size:10.0pt'>5. Understanding LSTM
  Networks (blog post overview)</span></a></td>
  <td class=xl69></td>
  <td class=xl69></td>
  <td class=xl69></td>
 </tr>
 <tr height=20 style='height:15.0pt'>
  <td rowspan=7 height=180 class=xl89 style='border-bottom:.5pt solid black;
  height:135.0pt;border-top:none'>4</td>
  <td class=xl72 width=343 style='width:257pt'>¡¡</td>
  <td class=xl123 width=467 style='width:350pt'>Suggested Readings (CNNs):</td>
  <td rowspan=7 class=xl89 style='border-bottom:.5pt solid black;border-top:
  none'>Assignment 2 out</td>
  <td class=xl69></td>
  <td class=xl69></td>
  <td class=xl69></td>
 </tr>
 <tr height=19 style='height:14.25pt'>
  <td height=19 class=xl73 width=343 style='height:14.25pt;width:257pt'>¡¡</td>
  <td class=xl70 width=467 style='width:350pt'><a
  href="https://arxiv.org/abs/1408.5882.pdf" target="_parent"><span
  style='font-size:10.0pt'>1. Convolutional Neural Networks for Sentence
  Classification</span></a></td>
  <td class=xl69></td>
  <td class=xl69></td>
  <td class=xl69></td>
 </tr>
 <tr height=34 style='height:25.5pt'>
  <td height=34 class=xl72 width=343 style='height:25.5pt;width:257pt'>Looking
  within a word (Subword modelling)</td>
  <td class=xl70 width=467 style='width:350pt'><a
  href="https://arxiv.org/abs/1207.0580" target="_parent"><span
  style='font-size:10.0pt'>2. Improving neural networks by preventing
  co-adaptation of feature detectors</span></a></td>
  <td class=xl69></td>
  <td class=xl69></td>
  <td class=xl69></td>
 </tr>
 <tr height=19 style='height:14.25pt'>
  <td height=19 class=xl73 width=343 style='height:14.25pt;width:257pt'>ConvNets
  for NLP&nbsp;</td>
  <td class=xl70 width=467 style='width:350pt'><a
  href="https://arxiv.org/pdf/1404.2188.pdf" target="_parent"><span
  style='font-size:10.0pt'>3. A Convolutional Neural Network for Modelling
  Sentences</span></a></td>
  <td class=xl69></td>
  <td class=xl69></td>
  <td class=xl69></td>
 </tr>
 <tr height=20 style='height:15.0pt'>
  <td height=20 class=xl80 width=343 style='height:15.0pt;width:257pt'>¡¡</td>
  <td class=xl123 width=467 style='width:350pt'>Suggested readings:</td>
  <td class=xl69></td>
  <td class=xl69></td>
  <td class=xl69></td>
 </tr>
 <tr height=34 style='height:25.5pt'>
  <td height=34 class=xl80 width=343 style='height:25.5pt;width:257pt'>¡¡</td>
  <td class=xl70 width=467 style='width:350pt'><a
  href="https://arxiv.org/abs/1604.00788.pdf" target="_parent"><span
  style='font-size:10.0pt'>1. Achieving Open Vocabulary Neural Machine
  Translation with Hybrid Word-Character Models</span></a></td>
  <td class=xl69></td>
  <td class=xl69></td>
  <td class=xl69></td>
 </tr>
 <tr height=34 style='height:25.5pt'>
  <td height=34 class=xl81 width=343 style='height:25.5pt;width:257pt'>¡¡</td>
  <td class=xl71 width=467 style='width:350pt'><a
  href="https://arxiv.org/pdf/1808.09943.pdf" target="_parent"><span
  style='font-size:10.0pt'>2. Revisiting Character-Based Neural Machine
  Translation with Capacity and Compression</span></a></td>
  <td class=xl69></td>
  <td class=xl69></td>
  <td class=xl69></td>
 </tr>
 <tr height=20 style='height:15.0pt'>
  <td rowspan=8 height=169 class=xl89 style='border-bottom:.5pt solid black;
  height:126.75pt;border-top:none'>5</td>
  <td class=xl72 width=343 style='width:257pt'>¡¡</td>
  <td class=xl123 width=467 style='width:350pt'>Suggested Readings (Dependency
  Parsing):</td>
  <td rowspan=8 class=xl95 style='border-bottom:.5pt solid black;border-top:
  none'>¡¡</td>
  <td class=xl69></td>
  <td class=xl69></td>
  <td class=xl69></td>
 </tr>
 <tr height=19 style='height:14.25pt'>
  <td height=19 class=xl73 width=343 style='height:14.25pt;width:257pt'>¡¡</td>
  <td class=xl70 width=467 style='width:350pt'><a
  href="https://www.aclweb.org/anthology/W/W04/W04-0308.pdf" target="_parent"><span
  style='font-size:10.0pt'>1. Incrementality in Deterministic Dependency
  Parsing</span></a></td>
  <td class=xl69></td>
  <td class=xl69></td>
  <td class=xl69></td>
 </tr>
 <tr height=19 style='height:14.25pt'>
  <td height=19 class=xl75 width=343 style='height:14.25pt;width:257pt'>¡¡</td>
  <td class=xl70 width=467 style='width:350pt'><a
  href="https://www.emnlp2014.org/papers/pdf/EMNLP2014082.pdf" target="_parent"><span
  style='font-size:10.0pt'>2. A Fast and Accurate Dependency Parser using
  Neural Networks</span></a></td>
  <td class=xl69></td>
  <td class=xl69></td>
  <td class=xl69></td>
 </tr>
 <tr height=19 style='height:14.25pt'>
  <td height=19 class=xl72 width=343 style='height:14.25pt;width:257pt'>The
  Syntax, Semantics Interface:&nbsp;</td>
  <td class=xl70 width=467 style='width:350pt'><a
  href="http://www.morganclaypool.com/doi/abs/10.2200/S00169ED1V01Y200901HLT002"
  target="_parent"><span style='font-size:10.0pt'>3. Dependency Parsing</span></a></td>
  <td class=xl69></td>
  <td class=xl69></td>
  <td class=xl69></td>
 </tr>
 <tr height=34 style='height:25.5pt'>
  <td height=34 class=xl73 width=343 style='height:25.5pt;width:257pt'>Dependency
  Parsing, Constituency Parsing, and Combinatory Categorial Grammars</td>
  <td class=xl70 width=467 style='width:350pt'><a
  href="https://arxiv.org/pdf/1603.06042.pdf" target="_parent"><span
  style='font-size:10.0pt'>4. Globally Normalized Transition-Based Neural
  Networks</span></a></td>
  <td class=xl69></td>
  <td class=xl69></td>
  <td class=xl69></td>
 </tr>
 <tr height=20 style='height:15.0pt'>
  <td height=20 class=xl75 width=343 style='height:15.0pt;width:257pt'>¡¡</td>
  <td class=xl123 width=467 style='width:350pt'>Suggested Readings
  (Constituency Parsing):</td>
  <td class=xl69></td>
  <td class=xl69></td>
  <td class=xl69></td>
 </tr>
 <tr height=19 style='height:14.25pt'>
  <td height=19 class=xl75 width=343 style='height:14.25pt;width:257pt'>¡¡</td>
  <td class=xl70 width=467 style='width:350pt'><a
  href="http://www.aclweb.org/anthology/P13-1045" target="_parent"><span
  style='font-size:10.0pt'>1. Parsing with Compositional Vector Grammars.</span></a></td>
  <td class=xl69></td>
  <td class=xl69></td>
  <td class=xl69></td>
 </tr>
 <tr height=19 style='height:14.25pt'>
  <td height=19 class=xl76 width=343 style='height:14.25pt;width:257pt'>¡¡</td>
  <td class=xl71 width=467 style='width:350pt'><a
  href="https://arxiv.org/pdf/1805.01052.pdf" target="_parent"><span
  style='font-size:10.0pt'>2. Constituency Parsing with a Self-Attentive
  Encoder</span></a></td>
  <td class=xl69></td>
  <td class=xl69></td>
  <td class=xl69></td>
 </tr>
 <tr height=20 style='height:15.0pt'>
  <td rowspan=6 height=190 class=xl89 style='border-bottom:.5pt solid black;
  height:142.5pt;border-top:none'>6</td>
  <td class=xl72 width=343 style='width:257pt'>¡¡</td>
  <td class=xl123 width=467 style='width:350pt'>Suggested Readings:</td>
  <td rowspan=6 class=xl95 style='border-bottom:.5pt solid black;border-top:
  none'>¡¡</td>
  <td class=xl69></td>
  <td class=xl69></td>
  <td class=xl69></td>
 </tr>
 <tr height=34 style='height:25.5pt'>
  <td height=34 class=xl72 width=343 style='height:25.5pt;width:257pt'>NLU
  beyond a sentence</td>
  <td class=xl70 width=467 style='width:350pt'><a
  href="https://arxiv.org/pdf/1409.3215.pdf" target="_parent"><span
  style='font-size:10.0pt'>1. Sequence to Sequence Learning with Neural
  Networks (original seq2seq NMT paper)</span></a></td>
  <td class=xl69></td>
  <td class=xl69></td>
  <td class=xl69></td>
 </tr>
 <tr height=34 style='height:25.5pt'>
  <td height=34 class=xl73 width=343 style='height:25.5pt;width:257pt'>Seq2Seq
  and Attention</td>
  <td class=xl70 width=467 style='width:350pt'><a
  href="https://arxiv.org/pdf/1211.3711.pdf" target="_parent"><span
  style='font-size:10.0pt'>2. Sequence Transduction with Recurrent Neural
  Networks (early seq2seq speech recognition paper)</span></a></td>
  <td class=xl69></td>
  <td class=xl69></td>
  <td class=xl69></td>
 </tr>
 <tr height=34 style='height:25.5pt'>
  <td height=34 class=xl73 width=343 style='height:25.5pt;width:257pt'>Case
  Study: Sentence Similarity, Textual Entailment and Machine Comprehension</td>
  <td class=xl70 width=467 style='width:350pt'><a
  href="https://arxiv.org/pdf/1409.0473.pdf" target="_parent"><span
  style='font-size:10.0pt'>3. Neural Machine Translation by Jointly Learning to
  Align and Translate (original seq2seq+attention paper)</span></a></td>
  <td class=xl69></td>
  <td class=xl69></td>
  <td class=xl69></td>
 </tr>
 <tr height=34 style='height:25.5pt'>
  <td height=34 class=xl75 width=343 style='height:25.5pt;width:257pt'>¡¡</td>
  <td class=xl70 width=467 style='width:350pt'><a
  href="https://distill.pub/2016/augmented-rnns/" target="_parent"><span
  style='font-size:10.0pt'>4. Attention and Augmented Recurrent Neural Networks
  (blog post overview)</span></a></td>
  <td class=xl69></td>
  <td class=xl69></td>
  <td class=xl69></td>
 </tr>
 <tr height=34 style='height:25.5pt'>
  <td height=34 class=xl76 width=343 style='height:25.5pt;width:257pt'>¡¡</td>
  <td class=xl71 width=467 style='width:350pt'><a
  href="https://arxiv.org/pdf/1703.03906.pdf" target="_parent"><span
  style='font-size:10.0pt'>5. Massive Exploration of Neural Machine Translation
  Architectures (practical advice for hyperparameter choices)</span></a></td>
  <td class=xl69></td>
  <td class=xl69></td>
  <td class=xl69></td>
 </tr>
 <tr height=20 style='height:15.0pt'>
  <td rowspan=3 height=73 class=xl89 style='border-bottom:.5pt solid black;
  height:54.75pt;border-top:none'>7</td>
  <td rowspan=3 class=xl106 width=343 style='border-bottom:.5pt solid black;
  border-top:none;width:257pt'>Predicate Argument Structures <br>
    <font class="font10">(Semantic Role Labelling, Frame Semantics, etc.)</font></td>
  <td class=xl123 width=467 style='width:350pt'>Suggested Reading</td>
  <td rowspan=3 class=xl89 style='border-bottom:.5pt solid black;border-top:
  none'>Assignment 3 out</td>
  <td class=xl69></td>
  <td class=xl69></td>
  <td class=xl69></td>
 </tr>
 <tr height=19 style='height:14.25pt'>
  <td height=19 class=xl70 width=467 style='height:14.25pt;width:350pt'><a
  href="https://web.stanford.edu/~jurafsky/slp3/20.pdf" target="_parent"><span
  style='font-size:10.0pt'>1. Semantic Role Labelling chapter of Jurafsky and
  Martin</span></a></td>
  <td class=xl69></td>
  <td class=xl69></td>
  <td class=xl69></td>
 </tr>
 <tr height=34 style='height:25.5pt'>
  <td height=34 class=xl71 width=467 style='height:25.5pt;width:350pt'><a
  href="http://aclweb.org/anthology/P18-2058" target="_parent"><span
  style='font-size:10.0pt'>2. Jointly Predicting Predicates and Arguments in
  Neural Semantic Role Labeling</span></a></td>
  <td class=xl69></td>
  <td class=xl69></td>
  <td class=xl69></td>
 </tr>
 <tr height=20 style='height:15.0pt'>
  <td rowspan=3 height=58 class=xl89 style='border-bottom:.5pt solid black;
  height:43.5pt;border-top:none'>8</td>
  <td rowspan=3 class=xl99 width=343 style='border-bottom:.5pt solid black;
  border-top:none;width:257pt'>Logical Representations of Language and
  Reasoning</td>
  <td class=xl123 width=467 style='width:350pt'>Suggested Readings:</td>
  <td rowspan=3 class=xl95 style='border-bottom:.5pt solid black;border-top:
  none'>¡¡</td>
  <td class=xl69></td>
  <td class=xl69></td>
  <td class=xl69></td>
 </tr>
 <tr height=19 style='height:14.25pt'>
  <td height=19 class=xl70 width=467 style='height:14.25pt;width:350pt'><a
  href="https://web.stanford.edu/~jurafsky/slp3/16.pdf" target="_parent"><span
  style='font-size:10.0pt'>1. Logical Representations chapter of Jurafsky and
  Martin</span></a></td>
  <td class=xl69></td>
  <td class=xl69></td>
  <td class=xl69></td>
 </tr>
 <tr height=19 style='height:14.25pt'>
  <td height=19 class=xl82 width=467 style='height:14.25pt;width:350pt'>¡¡</td>
  <td class=xl69></td>
  <td class=xl69></td>
  <td class=xl69></td>
 </tr>
 <tr height=20 style='height:15.0pt'>
  <td rowspan=2 height=39 class=xl102 style='border-bottom:.5pt solid black;
  height:29.25pt;border-top:none'>¡¡</td>
  <td class=xl83 width=343 style='width:257pt'>Practical Tips for Final
  Projects</td>
  <td class=xl125 width=467 style='width:350pt'>Suggested Readings:</td>
  <td rowspan=2 class=xl104 style='border-bottom:.5pt solid black;border-top:
  none'>Project Proposal</td>
  <td class=xl69></td>
  <td class=xl69></td>
  <td class=xl69></td>
 </tr>
 <tr height=19 style='height:14.25pt'>
  <td height=19 class=xl84 width=343 style='height:14.25pt;width:257pt'>(by
  TAs)</td>
  <td class=xl85 width=467 style='width:350pt'><a
  href="https://www.deeplearningbook.org/contents/guidelines.html"
  target="_parent"><span style='font-size:10.0pt'>1. Practical Methodology
  (Deep Learning book chapter)</span></a></td>
  <td class=xl69></td>
  <td class=xl69></td>
  <td class=xl69></td>
 </tr>
 <tr height=20 style='height:15.0pt'>
  <td rowspan=9 height=188 class=xl89 style='border-bottom:.5pt solid black;
  height:141.0pt;border-top:none'>9</td>
  <td rowspan=9 class=xl98 width=343 style='border-bottom:.5pt solid black;
  border-top:none;width:257pt'>Transformers and Contextual Word Representations
  (BERT, etc.)</td>
  <td class=xl123 width=467 style='width:350pt'>Suggested Readings:</td>
  <td rowspan=9 class=xl95 style='border-bottom:.5pt solid black;border-top:
  none'>¡¡</td>
  <td class=xl69></td>
  <td class=xl69></td>
  <td class=xl69></td>
 </tr>
 <tr height=19 style='height:14.25pt'>
  <td height=19 class=xl70 width=467 style='height:14.25pt;width:350pt'><a
  href="https://arxiv.org/abs/1706.03762.pdf" target="_parent"><span
  style='font-size:10.0pt'>1. Attention Is All You Need</span></a></td>
  <td class=xl69></td>
  <td class=xl69></td>
  <td class=xl69></td>
 </tr>
 <tr height=19 style='height:14.25pt'>
  <td height=19 class=xl70 width=467 style='height:14.25pt;width:350pt'><a
  href="https://jalammar.github.io/illustrated-transformer/" target="_parent"><span
  style='font-size:10.0pt'>2. The Illustrated Transformer</span></a></td>
  <td class=xl69></td>
  <td class=xl69></td>
  <td class=xl69></td>
 </tr>
 <tr height=19 style='height:14.25pt'>
  <td height=19 class=xl70 width=467 style='height:14.25pt;width:350pt'><a
  href="https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html"
  target="_parent"><span style='font-size:10.0pt'>3. Transformer (Google AI
  blog post)</span></a></td>
  <td class=xl69></td>
  <td class=xl69></td>
  <td class=xl69></td>
 </tr>
 <tr height=19 style='height:14.25pt'>
  <td height=19 class=xl70 width=467 style='height:14.25pt;width:350pt'><a
  href="https://arxiv.org/pdf/1607.06450.pdf" target="_parent"><span
  style='font-size:10.0pt'>4. Layer Normalization</span></a></td>
  <td class=xl69></td>
  <td class=xl69></td>
  <td class=xl69></td>
 </tr>
 <tr height=20 style='height:15.0pt'>
  <td height=20 class=xl123 width=467 style='height:15.0pt;width:350pt'>Suggested
  Readings:</td>
  <td class=xl69></td>
  <td class=xl69></td>
  <td class=xl69></td>
 </tr>
 <tr height=34 style='height:25.5pt'>
  <td height=34 class=xl70 width=467 style='height:25.5pt;width:350pt'><a
  href="https://arxiv.org/pdf/1810.04805.pdf" target="_parent"><span
  style='font-size:10.0pt'>1. BERT: Pre-training of Deep Bidirectional
  Transformers for Language Understanding</span></a></td>
  <td class=xl69></td>
  <td class=xl69></td>
  <td class=xl69></td>
 </tr>
 <tr height=19 style='height:14.25pt'>
  <td height=19 class=xl70 width=467 style='height:14.25pt;width:350pt'><a
  href="https://arxiv.org/abs/1902.06006.pdf" target="_parent"><span
  style='font-size:10.0pt'>2. Contextual Word Representations: A Contextual
  Introduction.</span></a></td>
  <td class=xl69></td>
  <td class=xl69></td>
  <td class=xl69></td>
 </tr>
 <tr height=19 style='height:14.25pt'>
  <td height=19 class=xl71 width=467 style='height:14.25pt;width:350pt'><a
  href="http://jalammar.github.io/illustrated-bert/" target="_parent"><span
  style='font-size:10.0pt'>3. The Illustrated BERT, ELMo, and co.</span></a></td>
  <td class=xl69></td>
  <td class=xl69></td>
  <td class=xl69></td>
 </tr>
 <tr height=20 style='height:15.0pt'>
  <td rowspan=5 height=96 class=xl89 style='border-bottom:.5pt solid black;
  height:72.0pt;border-top:none'>10</td>
  <td class=xl73 width=343 style='width:257pt'>¡¡</td>
  <td class=xl123 width=467 style='width:350pt'>Suggested Readings:</td>
  <td rowspan=5 class=xl95 style='border-bottom:.5pt solid black;border-top:
  none'>¡¡</td>
  <td class=xl69></td>
  <td class=xl69></td>
  <td class=xl69></td>
 </tr>
 <tr height=19 style='height:14.25pt'>
  <td height=19 class=xl72 width=343 style='height:14.25pt;width:257pt'>Natural
  Language Generation</td>
  <td class=xl70 width=467 style='width:350pt'><a
  href="https://arxiv.org/abs/1904.09751.pdf" target="_parent"><span
  style='font-size:10.0pt'>1. The Curious Case of Neural Text Degeneration.</span></a></td>
  <td class=xl69></td>
  <td class=xl69></td>
  <td class=xl69></td>
 </tr>
 <tr height=19 style='height:14.25pt'>
  <td height=19 class=xl73 width=343 style='height:14.25pt;width:257pt'>Case
  Study: Summarization and Conversation Modelling</td>
  <td class=xl70 width=467 style='width:350pt'><a
  href="https://arxiv.org/abs/1704.04368.pdf" target="_parent"><span
  style='font-size:10.0pt'>2. Get To The Point: Summarization with
  Pointer-Generator Networks.</span></a></td>
  <td class=xl69></td>
  <td class=xl69></td>
  <td class=xl69></td>
 </tr>
 <tr height=19 style='height:14.25pt'>
  <td height=19 class=xl75 width=343 style='height:14.25pt;width:257pt'>¡¡</td>
  <td class=xl70 width=467 style='width:350pt'><a
  href="https://arxiv.org/abs/1805.04833.pdf" target="_parent"><span
  style='font-size:10.0pt'>3. Hierarchical Neural Story Generation.</span></a></td>
  <td class=xl69></td>
  <td class=xl69></td>
  <td class=xl69></td>
 </tr>
 <tr height=19 style='height:14.25pt'>
  <td height=19 class=xl76 width=343 style='height:14.25pt;width:257pt'>¡¡</td>
  <td class=xl71 width=467 style='width:350pt'><a
  href="https://arxiv.org/abs/1603.08023.pdf" target="_parent"><span
  style='font-size:10.0pt'>4. How NOT To Evaluate Your Dialogue System.</span></a></td>
  <td class=xl69></td>
  <td class=xl69></td>
  <td class=xl69></td>
 </tr>
 <tr height=20 style='height:15.0pt'>
  <td rowspan=4 height=77 class=xl89 style='border-bottom:.5pt solid black;
  height:57.75pt;border-top:none'>11</td>
  <td rowspan=4 class=xl98 width=343 style='border-bottom:.5pt solid black;
  border-top:none;width:257pt'>Modelling and tracking entities: NER,
  coreference and information extraction (entity and relation extraction)</td>
  <td class=xl123 width=467 style='width:350pt'>Suggested Readings:</td>
  <td rowspan=4 class=xl95 style='border-bottom:.5pt solid black;border-top:
  none'>¡¡</td>
  <td class=xl69></td>
  <td class=xl69></td>
  <td class=xl69></td>
 </tr>
 <tr height=19 style='height:14.25pt'>
  <td height=19 class=xl70 width=467 style='height:14.25pt;width:350pt'><a
  href="https://web.stanford.edu/~jurafsky/slp3/22.pdf" target="_parent"><span
  style='font-size:10.0pt'>1. Coreference Resolution chapter of Jurafsky and
  Martin</span></a></td>
  <td class=xl69></td>
  <td class=xl69></td>
  <td class=xl69></td>
 </tr>
 <tr height=19 style='height:14.25pt'>
  <td height=19 class=xl70 width=467 style='height:14.25pt;width:350pt'><a
  href="https://arxiv.org/pdf/1707.07045.pdf" target="_parent"><span
  style='font-size:10.0pt'>2. End-to-end Neural Coreference Resolution</span></a></td>
  <td class=xl69></td>
  <td class=xl69></td>
  <td class=xl69></td>
 </tr>
 <tr height=19 style='height:14.25pt'>
  <td height=19 class=xl71 width=467 style='height:14.25pt;width:350pt'><a
  href="https://web.stanford.edu/~jurafsky/slp3/18.pdf" target="_parent"><span
  style='font-size:10.0pt'>3. Information Extraction chapter of Jurafsky and
  Martin</span></a></td>
  <td class=xl69></td>
  <td class=xl69></td>
  <td class=xl69></td>
 </tr>
 <tr height=102 style='mso-height-source:userset;height:76.5pt'>
  <td rowspan=3 height=140 class=xl89 style='border-bottom:.5pt solid black;
  height:105.0pt;border-top:none'>12</td>
  <td rowspan=3 class=xl92 width=343 style='border-bottom:.5pt solid black;
  border-top:none;width:257pt'>Language + {Knowledge, Vision, Action, Speech}</td>
  <td rowspan=3 class=xl126 width=467 style='border-bottom:.5pt solid black;
  border-top:none;width:350pt'>Suggested Readings:</td>
  <td rowspan=3 class=xl95 style='border-bottom:.5pt solid black;border-top:
  none'>¡¡</td>
  <td class=xl69></td>
  <td class=xl69></td>
  <td class=xl69></td>
 </tr>
 <tr height=19 style='height:14.25pt'>
  <td height=19 class=xl69 style='height:14.25pt'></td>
  <td class=xl69></td>
  <td class=xl69></td>
 </tr>
 <tr height=19 style='height:14.25pt'>
  <td height=19 class=xl69 style='height:14.25pt'></td>
  <td class=xl69></td>
  <td class=xl69></td>
 </tr>
 <tr height=122 style='mso-height-source:userset;height:91.5pt'>
  <td rowspan=3 height=160 class=xl89 style='border-bottom:.5pt solid black;
  height:120.0pt;border-top:none'>13</td>
  <td rowspan=3 class=xl92 width=343 style='border-bottom:.5pt solid black;
  border-top:none;width:257pt'>Optional lecture: Analysis and Interpretability
  of Neural NLP</td>
  <td rowspan=3 class=xl129 width=467 style='border-bottom:.5pt solid black;
  border-top:none;width:350pt'>Suggested Readings:</td>
  <td rowspan=3 class=xl95 style='border-bottom:.5pt solid black;border-top:
  none'>¡¡</td>
  <td class=xl69></td>
  <td class=xl69></td>
  <td class=xl69></td>
 </tr>
 <tr height=19 style='height:14.25pt'>
  <td height=19 class=xl69 style='height:14.25pt'></td>
  <td class=xl69></td>
  <td class=xl69></td>
 </tr>
 <tr height=19 style='height:14.25pt'>
  <td height=19 class=xl69 style='height:14.25pt'></td>
  <td class=xl69></td>
  <td class=xl69></td>
 </tr>
 <tr height=19 style='height:14.25pt'>
  <td height=19 class=xl86 style='height:14.25pt'>¡¡</td>
  <td class=xl87>Final project presentation (or poster session)</td>
  <td class=xl88 width=467 style='width:350pt'>¡¡</td>
  <td class=xl86>¡¡</td>
  <td class=xl69></td>
  <td class=xl69></td>
  <td class=xl69></td>
 </tr>
 <tr height=19 style='height:14.25pt'>
  <td height=19 class=xl86 style='height:14.25pt'>¡¡</td>
  <td class=xl87>Final project report submission</td>
  <td class=xl88 width=467 style='width:350pt'>¡¡</td>
  <td class=xl86>¡¡</td>
  <td class=xl69></td>
  <td class=xl69></td>
  <td class=xl69></td>
 </tr>
 <![if supportMisalignedColumns]>
 <tr height=0 style='display:none'>
  <td width=97 style='width:73pt'></td>
  <td width=343 style='width:257pt'></td>
  <td width=467 style='width:350pt'></td>
  <td width=142 style='width:107pt'></td>
  <td width=72 style='width:54pt'></td>
  <td width=72 style='width:54pt'></td>
  <td width=72 style='width:54pt'></td>
 </tr>
 <![endif]>
</table>

___

## Course Project/Assignment

TBD

### Submission Instructions

TBD

## Materials
TBD
-   [Project Guidelines]
-   [Course Assignment: Part 1]

___

## Contact

You can ask questions on  [piazza](https://piazza.com/ethz.ch/spring2021/263500000). Please post questions there, so others can see them and share in the discussion. If you have questions which are not of general interest, please don’t hesitate to contact us directly.

|||
|:--|:--|
|Lecturer| &nbsp;&nbsp;&nbsp;&nbsp; [Mrinmaya Sachan](http://www.mrinmaya.io/)|
|Teaching Assistants| &nbsp;&nbsp;&nbsp;&nbsp; [Jiaoda Li](https://rycolab.io/authors/jiaoda/),&nbsp; [Shehzaad Dhuliawala](https://people.cs.umass.edu/~sdhuliawala/),&nbsp; [Yifan Hou](https://yifan-h.github.io/)|
|||
